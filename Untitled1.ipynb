{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install snscrape\n",
    "\n",
    "import snscrape.modules.twitter as sntwitter\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "maxTweets = 10\n",
    "keyword = 'https://www.searchenginejournal.com/facebook-ads-to-remove-28-day-attribution-model/382436/'\n",
    "users = []\n",
    "\n",
    "# adds a timeframe: TwitterSearchScraper(keyword + 'since:2015-12-17 until:2020-09-25')\n",
    "\n",
    "for i in enumerate(sntwitter.TwitterSearchScraper(keyword).get_items()) :\n",
    "        if i > maxTweets :\n",
    "            break\n",
    "        users.append(tweet.username)\n",
    "        \n",
    "# user urls\n",
    "userUrls = ['https://twitter.com/' + users[n] for n in range(len(users))]\n",
    "userUrls\n",
    "\n",
    "# creates a data frame\n",
    "df = pd.DataFrame(index = users, \n",
    "                  columns = ['Full name',\n",
    "                             'Username',\n",
    "                             'Website',\n",
    "                             'Bio',\n",
    "                             'Location',\n",
    "                             'Following count',\n",
    "                             'Followers count',\n",
    "                             'Tweet count',\n",
    "                             'Profile image'\n",
    "                             ])\n",
    "\n",
    "################################################## soup doesn't work #################\n",
    "\n",
    "\n",
    "# tell Twitter your deal, add your e-mail and stuff\n",
    "for n in range(len(users)):\n",
    "    n = 1\n",
    "    headers = {\n",
    "        'User-Agent' : 'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/79.0.3945.88 Safari/537.36',\n",
    "        'Referer' : userUrls[n]\n",
    "    }\n",
    "\n",
    "    page = requests.get(userUrls[n], headers = headers)\n",
    "    soup = BeautifulSoup(page.content, 'lxml')\n",
    "    \n",
    "    # gets full name\n",
    "    spans = soup.find_all('span', attrs={'class':'css-901oao css-16my406 r-1qd0xha r-ad9z0x r-bcqeeo r-qvutc0'})\n",
    "    for span in spans:\n",
    "        print span.string\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
